---
layout: post
title:  "ML -- k近邻值"
categories: MachineLearning
tags: ML DL k-means
author: Chester Cheung
---

* content
{:toc}

k近邻值是一种基本的分类与回归方法，我们先学习分类的方法。k近邻值的输入是实例的特征向量，对应于特征空间中的特征向量；输出为实例的类别，可以取多类。



k近邻值通过训练集中给实例分类，从而对新的实例，可以根据其最临近的训练实例所属的类别，通过多数表决的方式进行预测。所以，**k近邻值不具有显式的学习过程**









给定训练集，对于新输入的实例点，在训练集中找到与这个点最近邻的k个实例，如果这k个实例中的多数属于某个类，那么就把该输入实例分到这个类中。我们常说，物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他/她身边的朋友入手，所谓观其友，而识其人。

![1](https://img-blog.csdnimg.cn/20190527102322646.png)

如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。


如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。



> ## k近邻值模型的三大基本要素


k值的选择，距离度量，分类决策规则是k近邻值法的三个基本要素。
在k近邻值中，当训练集、k值选择、距离度量、分类决策规则都确定后，对于任何一个新的输入实例，他所属的类唯一确定。这个就相当于在特征空间中划分出很多个子空间，可以确定子空间中每个店所属的类。



更形象一点，对于特征空间中的每个实例点xi，距离该点比其他店更近的所有点所组成的一个区域，叫做单元(cell)，每个训练实例点都有一个单元，所有训练实例点的所有单元对特征空间构成了一个完整的划分。而k近邻值法模型就是，将实例点xi的类yi作为其单元中所有点的类标记，这样，每个单元实例点的类别都是清楚的。



> ## 距离度量


在特征空间中，两个实例点的距离就是这两个点相似程度的反映，一般k近邻值使用的距离是欧式距离，但也可能是更一般的其他距离，比如Lp距离，Minkowski距离。



欧式距离：

![2](https://img-blog.csdnimg.cn/20190527102642228.png)

曼哈顿距离：

![3](https://img-blog.csdnimg.cn/20190527102705947.png)

更加通用的闵可夫斯基距离(Minkowski Distance)：

![4](https://img-blog.csdnimg.cn/20190527102752633.png)

**而且，要注意的是，由不同的距离度量所确定的最近邻点是不同的**


> ## 
对于k值的选取


如果选取的k值过小，相当于使用较小的邻域中的训练实例进行预测，只有与输入实例较接近的训练实例才会起到作用，如果此时邻近的实例点中恰巧有噪声，预测就会出错；总结就是，k值过小会使整体的模型变得更复杂，容易发生过拟合。



如果k值的选取过大，相当于用较大邻域中的训练实例进行预测，可以减小学习的估计误差，但是对较远的实例点也会起预测作用，使得预测产生错误；总结就是，k值的增大可以使整体模型变得更简单。