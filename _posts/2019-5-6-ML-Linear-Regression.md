---
layout: post
title:  "ML Linear Regrssion"
categories: Machine Learning
tags: ML AI Regression
author: Chester Cheung
---

* content
{:toc}


线性回归作为回归算法中的较为基础的算法，所以先从线型回归入手。



参考论文：[http://cs229.stanford.edu/notes-spring2019/cs229-notes1.pdf

](http://cs229.stanford.edu/notes-spring2019/cs229-notes1.pdf)

我们先来看一个简单的例子，通过房间的面积和卧室的数目来预测房价。那么这里的X就是属于R^2的维向量，我们把x1设为居住面积，x2设为卧室的数量(初期把问题简化，做到最小可实现)。



## 建立线性方程



为了进行机器学习，我们必须决定怎样表达预测值h，我们选用比较常用的一种表达方法，只要你有基本的数学基础，就一定不会陌生：

![1](https://img-blog.csdnimg.cn/2019042521163374.png)

上式中的“θ”是所谓的参数，通常叫做“权重”(weights)，是参数化回归方程从X到y的函数空间的映射的参数( the parameters parameterizing the space of linear functions mapping from X to Y)(看不懂就对了)，通俗讲就是每一个影响房价的因素在对价格的影响上面所占的比重。通过构造这样一个函数，来对房价进行较为准确的预测。为了简化这个过程，我们设定初始化的X0=1(X0即为截距项)，

![2](https://img-blog.csdnimg.cn/20190425212344876.png)

因此我们最后得到的就是这样的一个h关于x的线型方程，看看，清晰明了吧，所预测的曲线在图像中也是一条直线。



等式右边看到的θ和x都是向量，这里的d是我们输入的数据的数量(即训练集中样本的数量)。现在到了关键时候，加入有了一个输入的训练集，我们怎么样才能算出或者学习得到这里每一个影响因素即每一个标签的权重θ呢？这才是得出我们线型方程的核心。

## 计算线型方程中的权重θ

为了解决这个问题，我们要定义一个函数来得到这里的θ，函数的功能是测量对于每一个θ的h(x)和他相应的Y之间有多接近：

![3](https://img-blog.csdnimg.cn/20190425212911231.png)

之前学习过线性回归的同学不难看出，上式就是熟悉的最小二乘成本函数以及所对应的普通最小二乘回归模型(ordinary least squares regression model)。接下来我们就正式开始计算θ了。



我们这里需要选择合适的θ，以使J(θ)最小化，所以我们使用了一个搜索算法来找到一些初始猜测的θ，然后通过不断改变θ来使J(θ)变得更小，直到我们找到最小的θ值或接近最小。



这里有2中算法可以达到这个目的，我们先来看看梯度下降算法，他就是以一些初始的θ值开始，然后进行重复迭代：

![4](https://img-blog.csdnimg.cn/2019042521453381.png)

(这种迭代可以同时执行对所有θ的迭代：j = 0,1,2…d)，这里的α通常被叫做学习率(learning rate)，这就是一个原理非常简单而自然的算法，他可以朝着J最小值的方向不断进行迭代、覆盖。
为了实现这种算法，我们必须先求出等式右边的偏导数。还是为了达到最小可实现，我们先算出仅有一组样本(x,y) 的情况，所以我们暂时先忽略J的定义中的求和符号，然后就有了以下的推导：

![5](https://img-blog.csdnimg.cn/20190425221048140.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDM5MDE0NQ==,size_16,color_FFFFFF,t_70)

然后把这个式子带回到θ(j)的计算式中，然后就能得到单个的迭代规则：

![6](https://img-blog.csdnimg.cn/20190425221425721.png)

这种规则叫做最小均方算法(LMS, “least mean squares”)也被称作Windrow-Hoff学习算法，是一种应用为广泛的自适应滤波算法。他的优点是，每一次迭代所改变的程度都取决于偏差项(y(i) - h(x(i)))*Xj(i)，如果训练的样本的预测值和真实的y值的偏差很小，权重的改变就会很轻微甚至不变；相反，如果预测值和实际y值相差甚远，那么权重就会有大幅改变。
我们定义LMS算法是为了计算只有单个样本的数据，但是最终的数据不可能只有一个样本，所以我们还需要从最小可实现跨一步到整体数据上，我们这里有两种办法实现，第一种是用下列算法来代替：


![7](https://img-blog.csdnimg.cn/20190426080936698.png)

只要前面θ(j)迭代的式子弄懂了，这个新的式子不过是在前面的基础上进行了一些求和的处理，原理都是相通的。这种方法可以考虑到数据集中的每一个样本，对每一个样本数据的初始成本函数J进行迭代的梯度下降，这种算法叫做批量梯度下降法。



**注意**，梯度一般很容易受到局部最小值的影响，但是线型回归中的最优问题只有一个，没有其他的局部最优，所以梯度总是下降到全局极小(学习率α不能太大也不能太小，如果太大会使梯度超过最小值，在最小值附近徘徊；如果太小则梯度的迭代效率会很慢)。



## 梯度下降算法的分类



**梯度下降法的原理**：



梯度思想的three elements：**出发点、下降方向、下降步长**


出发点很重要，但是方向和步长才是关键！事实上，不同梯度的不同就是这两点的不同。
当梯度较大的时候，离最优解比较远，weight的更新速度比较快 ，到了梯度较小的地方，也就是离最优解比较近的地方，如果weight的更新速度还保持原来的速度，这样会很容易导致更新过度反而远离了最优解，进而导致在最优解附近来回震荡徘徊，这样显然不行。所以，在远离最优解的地方梯度大，靠近最优解的地方梯度小，我们让步长随着这个律动，就可以得到：W = W - λdW
这时的学习率λ是随着坡度的陡缓来变化的，别看他是个常数。



梯度下降一般分为批量梯度下降(Batch Gradient Descent，BGD)、随机梯度下降(Stochastic Gradient Descent，SGD)、小批量梯度下降(Mini-Batch Gradient Descent, MBGD)等，我们上面介绍的是其中的批量梯度下降，下面再来较为详细地讲解下三种算法的异同：



**批量梯度下降**



缺点上面已经介绍了，这里不再多说，只说明一下这种算法的优缺点：


优点：(1)一次迭代是对所有样本的计算，此时可利用矩阵进行操作，实现了并行
(2)由全数据集确定的方向能很好的代表样本总体，进而精准的表示朝向极值所在的方向。且当目标函数为凸函数时，这种算法一定能够得到全局最优解
缺点：当样本数据集很大时，每一步迭代都需要对所有样本进行计算，耗费相当的时间，过程会很慢；但是从迭代的次数上看，BGD迭代的次数相对还不太多。



**随机梯度下降**



这种算法是每次迭代会使用一个样本来对参数进行更新，使得训练的速度加快
